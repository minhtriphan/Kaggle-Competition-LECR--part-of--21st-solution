# -*- coding: utf-8 -*-
"""LECR-v17a-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gNOJHlSSwcssog4gKe1-gy5XDIL45Tdu

# Version v14a
* Non-leaky version of version v8b
"""

import os, gc, math, random, pickle, json, time
import numpy as np
import pandas as pd
from tqdm import tqdm
tqdm.pandas()

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import StratifiedGroupKFold, GroupKFold
from scipy.spatial.distance import cdist
from sklearn.neighbors import NearestNeighbors

import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler

import transformers
from transformers import AutoTokenizer, AutoConfig, AutoModel
from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, AdamW

import warnings
warnings.filterwarnings('ignore')

transformers.logging.set_verbosity_error()
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'true'

"""# Initial settings"""

class Config(object):
    # General setting
    competition_name = 'LECR'    # Learning Equality - Curriculum Recommendations
    seed = 2022
    env = 'vastai'
    ver = 'v17a'
    if env == 'colab':
        from google.colab import drive
        drive.mount('/content/drive')
    mode = 'train'
    use_tqdm = True
    use_log = True
    debug = False
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    # Model
    backbone = 'paraphrase-multilingual-mpnet-base-v4'    # 'sentence-transformers/all-MiniLM-L12-v2', 'paraphrase-multilingual-mpnet-base-v4', 'xlm-roberta-base'
    tokenizer = AutoTokenizer.from_pretrained(backbone)
    config = AutoConfig.from_pretrained(backbone)
    gradient_checkpointing = False
    # Add new token
    sep_token = '[LECR]'
    sep_token_id = tokenizer.vocab_size + 1
    special_tokens_dict = {'additional_special_tokens': [sep_token]}
    tokenizer.add_special_tokens(special_tokens_dict)
    # Data
    done_kfold_split = False
    processed_data = True
    nfolds = 5
    negative_sample = 5
    # Dataloader
    max_len = 32
    batch_size = 32 if not debug else 4
    num_workers = os.cpu_count()
    # For training
    training_folds = [0, 1, 2, 3, 4]
    apex = False
    gradient_checkpointing = True
    nepochs = 10
    gradient_accumulation_steps = 1
    max_grad_norm = 100
    # For validation
    val_check_interval = 1.
    thres = {
        'cosine': None,
        'num_k': 50,
    }
    # For AWP
    use_awp = True
    if use_awp:
        adv_lr = 1
        adv_eps = 1e-3
        adv_step = 1
        start_awp_epoch = 5
    else:
        start_awp_epoch = nepochs + 1
    # Optimizer
    lr = 1e-5
    weight_decay = 1e-2
    encoder_lr = 1e-5
    decoder_lr = 1e-3
    min_lr = 1e-6
    eps = 1e-6
    betas = (0.9, 0.999)
    # Scheduler
    scheduler_type = 'cosine'    # 'linear', 'cosine'
    if scheduler_type == 'cosine':
        num_cycles = 0.5
    num_warmup_steps = 0.
    batch_scheduler = True
    # Paths
    if env == 'colab':
        comp_data_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/comp_data'
        ext_data_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/ext_data'
        model_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/model'
        os.makedirs(os.path.join(model_dir, ver[:-1], ver[-1]), exist_ok = True)
    elif env == 'kaggle':
        comp_data_dir = ...
        ext_data_dir = ...
        model_dir = ...
    elif env == 'vastai':
        comp_data_dir = 'data'
        ext_data_dir = 'ext_data'
        model_dir = f'model'
        os.makedirs(os.path.join(model_dir, ver[:-1], ver[-1]), exist_ok = True)

cfg = Config()

"""# Set seed"""

def set_random_seed(seed, use_cuda = True):
    np.random.seed(seed) # cpu vars
    torch.manual_seed(seed) # cpu  vars
    random.seed(seed) # Python
    os.environ['PYTHONHASHSEED'] = str(seed) # Python hash building
    if use_cuda:
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # gpu vars
        torch.backends.cudnn.deterministic = True  #needed
        torch.backends.cudnn.benchmark = False

set_random_seed(cfg.seed)

"""# Prepare log"""

if cfg.use_log:
    import logging
    from imp import reload
    reload(logging)
    logging.basicConfig(
        level = logging.INFO,
        format = '%(asctime)s %(message)s',
        datefmt = '%H:%M:%S',
        handlers = [
            logging.FileHandler(f"train_{cfg.ver}_{time.strftime('%m%d_%H%M', time.localtime())}_seed_{cfg.seed}_folds_{''.join([str(i) for i in cfg.training_folds])}.log"),
            logging.StreamHandler()
        ]
    )

    logging.info(
        '\nver: {}\n'
        'backbone: {}\n'
        'env: {}\n'
        'seed: {}\n'
        'training_folds: {}\n'
        'max_len: {}\n'
        'batch_size: {}\n'
        'num_workers: {}\n'
        'nepochs: {}\n'
        'lr: {}\n'
        'weight_decay: {}\n'
        'start_awp_epoch: {}\n'.format(cfg.ver, cfg.backbone, cfg.env, cfg.seed, cfg.training_folds, 
                                       cfg.max_len, cfg.batch_size, cfg.num_workers, cfg.nepochs, 
                                       cfg.lr, cfg.weight_decay, cfg.start_awp_epoch)
    )

def print_log(cfg, message):
    if cfg.use_log:
        logging.info(message)
    else:
        print(message)

"""# Explore the data"""

content_df = pd.read_csv(os.path.join(cfg.comp_data_dir, 'content.csv'))
topics_df = pd.read_csv(os.path.join(cfg.comp_data_dir, 'topics.csv'))
correlations_df = pd.read_csv(os.path.join(cfg.comp_data_dir, 'correlations.csv'))

all_languages = sorted(list(set(topics_df['language'].tolist() + content_df['language'].tolist())))
cfg.languages_map = dict(zip(all_languages, range(len(all_languages))))

if not cfg.done_kfold_split:
    kf = GroupKFold(n_splits = cfg.nfolds)
    folds = list(kf.split(topics_df, groups = topics_df['channel']))
    topics_df['fold'] = -1

    for fold, (train_idx, val_idx) in enumerate(folds):
        topics_df.loc[val_idx, 'fold'] = fold
    topic2fold_split = topics_df[['id', 'fold']].set_index('id').to_dict()['fold']

    with open(os.path.join(cfg.ext_data_dir, f'topic2fold_{cfg.nfolds}split_stratifiedkfold.pkl'), 'wb') as f:
        pickle.dump(topic2fold_split, f)
else:
    with open(os.path.join(cfg.ext_data_dir, f'topic2fold_{cfg.nfolds}split_stratifiedkfold.pkl'), 'rb') as f:
        topic2fold_split = pickle.load(f)

correlations_df = pd.read_csv(os.path.join(cfg.comp_data_dir, 'correlations.csv'))
topics_df['fold'] = topics_df['id'].map(topic2fold_split)
correlations_df['fold'] = correlations_df['topic_id'].map(topic2fold_split)
content_df['fold'] = -1

for fold in range(cfg.nfolds):
    content_in_fold = set(correlations_df.loc[correlations_df['fold'] == fold, 'content_ids'].str.split().explode().tolist())
    content_not_in_fold = set(correlations_df.loc[correlations_df['fold'] != fold, 'content_ids'].str.split().explode().tolist())
    only_content_in_fold = content_in_fold - content_not_in_fold
    content_df.loc[content_df['id'].isin(only_content_in_fold), 'fold'] = fold

"""# Prepare the data"""

def process_data(cfg, df, is_content = False):
    # Fill NaN values in the title and description columns
    df['title'] = df['title'].fillna(' ')
    df['description'] = df['description'].fillna(' ')
    if is_content:
        df['text'] = df['text'].fillna(' ')

    # Encode the language
    df['encoded_language'] = df['language'].map(cfg.languages_map)
    return df

topics_df = process_data(cfg, topics_df)
content_df = process_data(cfg, content_df, is_content = True)

"""# Helper functions from the host"""

class Topic:
    def __init__(self, topic_id):
        self.id = topic_id

    @property
    def parent(self):
        parent_id = topics_df.loc[self.id].parent
        if pd.isna(parent_id):
            return None
        else:
            return Topic(parent_id)

    @property
    def ancestors(self):
        ancestors = []
        parent = self.parent
        while parent is not None:
            ancestors.append(parent)
            parent = parent.parent
        return ancestors

    @property
    def siblings(self):
        if not self.parent:
            return []
        else:
            return [topic for topic in self.parent.children if topic != self]

    @property
    def content(self):
        if self.id in correlations_df.index:
            return [ContentItem(content_id) for content_id in correlations_df.loc[self.id].content_ids.split()]
        else:
            return tuple([]) if self.has_content else []

    def get_breadcrumbs(self, separator=" >> ", include_self=True, include_root=True):
        ancestors = self.ancestors
        if include_self:
            ancestors = [self] + ancestors
        if not include_root:
            ancestors = ancestors[:-1]
        return separator.join([a.input_text for a in ancestors])
    
    def get_near_ancestors(self, separator=" >> ", how_near = 1, include_self = True):
        ancestors = self.ancestors
        if include_self:
            ancestors = [self] + ancestors[:how_near]
        return separator.join([a.input_text for a in ancestors])

    @property
    def children(self):
        return [Topic(child_id) for child_id in topics_df[topics_df.parent == self.id].index]

    def subtree_markdown(self, depth=0):
        markdown = "  " * depth + "- " + self.title + "\n"
        for child in self.children:
            markdown += child.subtree_markdown(depth=depth + 1)
        for content in self.content:
            markdown += ("  " * (depth + 1) + "- " + "[" + content.kind.title() + "] " + content.title) + "\n"
        return markdown

    def __eq__(self, other):
        if not isinstance(other, Topic):
            return False
        return self.id == other.id

    def __getattr__(self, name):
        return topics_df.loc[self.id][name]

    def __str__(self):
        return self.title
    
    def __repr__(self):
        return f"<Topic(id={self.id}, title=\"{self.title}\")>"


class ContentItem:
    def __init__(self, content_id):
        self.id = content_id

    @property
    def topics(self):
        return [Topic(topic_id) for topic_id in topics_df.loc[correlations_df[correlations_df.content_ids.str.contains(self.id)].index].index]

    def __getattr__(self, name):
        return content_df.loc[self.id][name]

    def __str__(self):
        return self.title
    
    def __repr__(self):
        return f"<ContentItem(id={self.id}, title=\"{self.title}\")>"

    def __eq__(self, other):
        if not isinstance(other, ContentItem):
            return False
        return self.id == other.id

    def get_all_breadcrumbs(self, separator=" >> ", include_root=True):
        breadcrumbs = []
        for topic in self.topics:
            new_breadcrumb = topic.get_breadcrumbs(separator=separator, include_root=include_root)
            if new_breadcrumb:
                new_breadcrumb = new_breadcrumb + separator + self.title
            else:
                new_breadcrumb = self.title
            breadcrumbs.append(new_breadcrumb)
        return breadcrumbs

"""# Process the topics_df data"""

topics_df = topics_df.set_index('id')
content_df = content_df.set_index('id')
correlations_df = correlations_df.set_index('topic_id')

topics_df['input_text'] = topics_df.apply(lambda x: cfg.sep_token.join([x['language'],
                                                                        x['title'],
                                                                        x['description']]), axis = 1)

content_df['input_text'] = content_df.apply(lambda x: cfg.sep_token.join([x['language'],
                                                                          x['title'],
                                                                          x['description'],
                                                                          x['text'],
                                                                          x['kind']]), axis = 1)

topics_df['text'] = topics_df['input_text'].values

for topic_id in tqdm(topics_df.index):
    topics_df.loc[topic_id, 'text'] = Topic(topic_id).get_breadcrumbs(separator = cfg.tokenizer.sep_token)
    
topics_df = topics_df.drop('input_text', axis = 1).rename(columns = {'text': 'input_text'})
    
topics_df = topics_df.reset_index()
content_df = content_df.reset_index()
correlations_df = correlations_df.reset_index()

topics_df = topics_df.drop(['title', 'description'], axis = 1)
content_df = content_df.drop(['title', 'description', 'text', 'kind'], axis = 1)

"""# Plain k-NN"""

class LECR_ComponentDataset(Dataset):
    def __init__(self, cfg, df):
        self.cfg = cfg
        self.input_text = df['input_text'].tolist()
        self.ids = df['id'].tolist()
        self.language = df['encoded_language'].tolist()

    def _tokenize(self, text):
        token = self.cfg.tokenizer(text,
                                   padding = 'max_length',
                                   max_length = cfg.max_len,
                                   truncation = True,
                                   return_attention_mask = True)
        return token

    def __len__(self):
        return len(self.input_text)

    def __getitem__(self, idx):
        ids = self.ids[idx]
        input_text = self.input_text[idx]
        language = self.language[idx]
        
        content_token = self._tokenize(input_text)
        
        return {
            'ids': ids,
            'input_ids': torch.tensor(content_token['input_ids'], dtype = torch.long),
            'attention_mask': torch.tensor(content_token['attention_mask'], dtype = torch.long),
            'language': torch.tensor(language, dtype = torch.long),
        }

class TextEmbedding(object):
    def __init__(self, cfg, df):
        self.cfg = cfg
        self.df = df
        
    def _prepare_materials(self):
        print_log(self.cfg, 'Preparing the dataloader...')
        dataset = LECR_ComponentDataset(cfg, self.df)
        dataloader = DataLoader(dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)
        
        print_log(self.cfg, 'Preparing the encoding model...')
        model = AutoModel.from_pretrained(self.cfg.backbone).to(self.cfg.device)
        model.resize_token_embeddings(len(self.cfg.tokenizer))
        return model, dataloader
    
    def _pooler(self, x, mask = None):
        if mask is not None:
            x = x * mask.unsqueeze(-1)
            return x.sum(dim = 1) / mask.sum(dim = -1, keepdims = True)
        else:
            return x.mean(dim = 1)
    
    def _embedding(self, model, dataloader):
        model.eval()
    
        ids = []
        embeddings = []
        languages = []
        
        if self.cfg.use_tqdm:
            tbar = tqdm(dataloader)
        else:
            tbar = dataloader

        for i, item in enumerate(tbar):
            batch_ids = item['ids']
            input_ids = item['input_ids'].to(self.cfg.device)
            attention_mask = item['attention_mask'].to(self.cfg.device)
            batch_languages = item['language']
            
            with torch.no_grad():
                with autocast(enabled = self.cfg.apex):
                    local_len = max(attention_mask.sum(axis = 1))
                    batch_embedding = model(input_ids[:,:local_len], attention_mask[:,:local_len]).last_hidden_state
                    batch_embedding = self._pooler(batch_embedding, mask = attention_mask[:,:local_len])
                        
            ids.append(batch_ids)
            embeddings.append(batch_embedding.cpu().numpy())
            languages.append(batch_languages.numpy())

        ids = np.concatenate(ids)
        embeddings = np.concatenate(embeddings)
        languages = np.concatenate(languages)
        return ids, embeddings, languages

    def fit(self):
        model, dataloader = self._prepare_materials()
        ids, embeddings, languages = self._embedding(model, dataloader)
        return ids, embeddings, languages

def metric_fn(y_pred_ids: pd.Series, y_true_ids: pd.Series, beta = 2, eps = 1e-15):
    true_ids = y_true_ids.str.split()
    pred_ids = y_pred_ids.str.split()
    score_list = []
    recall_list = []
    for true, pred in zip(true_ids.tolist(), pred_ids.tolist()):
        TP = (set(true) & set(pred))
        precision = len(TP) / len(pred)
        recall = len(TP) / len(true)
        f2 = (1 + beta**2) * (precision * recall) / ((beta**2) * precision + recall + eps)
        score_list.append(f2)
        recall_list.append(recall)
    score = sum(score_list) / len(score_list)
    recall_score = sum(recall_list) / len(recall_list)
    return score, recall_score

def metric_eachrow_fn(pred, true, beta = 2, eps = 1e-15):
    pred = pred.split()
    true = true.split()
    TP = (set(true) & set(pred))
    precision = len(TP) / len(pred)
    recall = len(TP) / len(true)
    f2 = (1 + beta**2) * (precision * recall) / ((beta**2) * precision + recall + eps)
    return f2

topic_data = topics_df[['id', 'input_text']].set_index('id').to_dict()['input_text']
content_data = content_df[['id', 'input_text']].set_index('id').to_dict()['input_text']

"""# Process the training data
* Attach the correlated topic_ids
* Encode the topic_ids
"""

correlations_df['content_ids'] = correlations_df['content_ids'].str.split()
correlations_df = correlations_df.explode('content_ids')
correlations_df['label'] = 1
correlations_df = correlations_df.reset_index(drop = True)
correlations_df

topic_class_map = dict(zip(np.sort(correlations_df['content_ids'].unique()), range(correlations_df['content_ids'].nunique())))
content_class_map = dict(zip(np.sort(correlations_df['topic_id'].unique()), range(correlations_df['topic_id'].nunique())))

data = correlations_df.copy()
data['topic_class'] = data['content_ids'].map(topic_class_map)
data['content_class'] = data['topic_id'].map(content_class_map)
data['fold'] = data['topic_id'].map(topic2fold_split)
data

"""# Design the dataloader"""

class LECRDataset(Dataset):
    def __init__(self, cfg, df, topic_data, content_data):
        self.cfg = cfg
        self.topic_id = df['topic_id'].tolist()
        self.content_id = df['content_ids'].tolist()
        self.topic_class = df['topic_class'].tolist()
        self.content_class = df['content_class'].tolist()
        self.label = df['label'].tolist()
        self.topic_data = topic_data
        self.content_data = content_data
        
    def _tokenize(self, text):
        token = self.cfg.tokenizer(text,
                                   padding = 'max_length',
                                   max_length = cfg.max_len,
                                   truncation = True,
                                   return_attention_mask = True)
        return token

    def __len__(self):
        return len(self.topic_id)

    def __getitem__(self, idx):
        topic_id = self.topic_id[idx]
        content_id = self.content_id[idx]
        topic_class = self.topic_class[idx]
        content_class = self.content_class[idx]
        label = self.label[idx]
        
        topic_input_text = self.topic_data[topic_id]
        content_input_text = self.content_data[content_id]

        topic_token = self._tokenize(topic_input_text)
        content_token = self._tokenize(content_input_text)
        
        return {
            'topic_input_ids': torch.tensor(topic_token['input_ids'], dtype = torch.long),
            'topic_attention_mask': torch.tensor(topic_token['attention_mask'], dtype = torch.long),
            
            'content_input_ids': torch.tensor(content_token['input_ids'], dtype = torch.long),
            'content_attention_mask': torch.tensor(content_token['attention_mask'], dtype = torch.long),
            
            'topic_class': torch.tensor(topic_class, dtype = torch.long),
            'content_class': torch.tensor(content_class, dtype = torch.long),
            'label': torch.tensor(label, dtype = torch.float),
        }

"""# Model

* Focal loss
"""

class FocalLoss(nn.Module):
    '''
    This criterion is a implemenation of Focal Loss, which is proposed in 
        Focal Loss for Dense Object Detection.

            Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class])

        The losses are averaged across observations for each minibatch.
        
        From https://www.kaggle.com/code/zeta1996/pytorch-lightning-arcface-focal-loss

        Args:
            alpha(1D Tensor, Variable) : the scalar factor for this criterion
            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), 
                                   putting more focus on hard, misclassiﬁed examples
            size_average(bool): By default, the losses are averaged over observations for each minibatch.
                                However, if the field size_average is set to False, the losses are
                                instead summed for each minibatch.
    '''
    def __init__(self, class_num = 61517, alpha = None, gamma = 2, size_average = True):
        super(FocalLoss, self).__init__()
        if alpha is None:
            self.alpha = Variable(torch.ones(class_num, 1))
        else:
            if isinstance(alpha, Variable):
                self.alpha = alpha
            else:
                self.alpha = Variable(alpha)
        self.gamma = gamma
        self.class_num = class_num
        self.size_average = size_average

    def forward(self, inputs, targets):
        N = inputs.size(0)
        C = inputs.size(1)
        P = F.softmax(inputs, dim = 1)

        class_mask = inputs.data.new(N, C).fill_(0)
        class_mask = Variable(class_mask)
        ids = targets.view(-1, 1)
        class_mask.scatter_(1, ids.data, 1.)

        if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.to(cfg.device)
        alpha = self.alpha[ids.data.view(-1)]

        probs = (P * class_mask).sum(1).view(-1,1)

        log_p = probs.log()
        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p

        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()
        return loss

"""* Arcface head"""

class ArcMarginProduct(nn.Module):
    '''
    From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py
    Implement of large margin arc distance:
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        s: norm of input feature
        m: margin
        cos(theta + m)
    '''
    def __init__(
        self,
        in_features: int,
        out_features: int,
        s: float,
        m: float,
        easy_margin: bool,
        ls_eps: float,
    ):
        super(ArcMarginProduct, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.ls_eps = ls_eps  # label smoothing
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

        self.easy_margin = easy_margin
        self.cos_m = math.cos(m)
        self.sin_m = math.sin(m)
        self.th = math.cos(math.pi - m)
        self.mm = math.sin(math.pi - m) * m

    def forward(self, input: torch.Tensor, label: torch.Tensor) -> torch.Tensor:
        # --------------------------- cos(theta) & phi(theta) ---------------------
        device = input.device
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        # Enable 16 bit precision
        cosine = cosine.to(torch.float32)

        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))
        phi = cosine * self.cos_m - sine * self.sin_m
        if self.easy_margin:
            phi = torch.where(cosine > 0, phi, cosine)
        else:
            phi = torch.where(cosine > self.th, phi, cosine - self.mm)
        # --------------------------- convert label to one-hot ---------------------
        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')
        one_hot = torch.zeros(cosine.size(), device = device)
        one_hot.scatter_(1, label.view(-1, 1).long(), 1)
        if self.ls_eps > 0:
            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features
        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------
        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)
        output *= self.s

        return output

class ContrastiveLoss(nn.Module):
    def __init__(self, margin = 0):
        super().__init__()
        self.margin = margin

    def forward(self, output1, output2, targets):
        d = 1 - nn.CosineSimilarity()(output1, output2)
        loss = torch.mean(0.5 * targets.float() * d.pow(2) + \
                          0.5 * (1 - targets.float()) * F.relu(self.margin - d).pow(2))
        return loss

"""* Pooling"""

class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()
        
    def forward(self, x, mask = None):
        if mask is not None:
            x = x * mask.unsqueeze(-1)
            return x.sum(dim = 1) / mask.sum(dim = -1, keepdims = True)
        else:
            return x.mean(dim = 1)

"""* Main model"""

class LECRModel(nn.Module):
    def __init__(self, cfg):
        super(LECRModel, self).__init__()
        self.cfg = cfg
        self.backbone = AutoModel.from_pretrained(cfg.backbone)
        self.backbone.resize_token_embeddings(len(cfg.tokenizer))
        if self.cfg.gradient_checkpointing:
            self.backbone.gradient_checkpointing_enable()
        self.pooler = MeanPooling()
        
        self.arcface_topic = ArcMarginProduct(
            in_features = cfg.config.hidden_size,
            out_features = 154047,
            s = 10.,
            m = 0.5,
            easy_margin = True,
            ls_eps = 1e-6
        )
        self.arcface_content = ArcMarginProduct(
            in_features = cfg.config.hidden_size,
            out_features = 61517,
            s = 10.,
            m = 0.5,
            easy_margin = True,
            ls_eps = 1e-6
        )
        
    def _feature_generator(self, input_ids, attention_mask):
        local_len = max(attention_mask.sum(axis = 1))
        output_backbone = self.backbone(input_ids[:,:local_len], attention_mask = attention_mask[:,:local_len])
        embedding = self.pooler(output_backbone.last_hidden_state, mask = attention_mask[:,:local_len])
        return embedding
    
    def loss_fn(self, topic_embedding, content_embedding, 
                topic_output, content_output, topic_classs, content_class, label):
        contrastive_loss = ContrastiveLoss()(topic_embedding, content_embedding, label)
        # Topic Arcface loss
        topic_arcface_loss = FocalLoss(class_num = 154047)(topic_output, topic_classs)
        # Content Arcface loss
        content_arcface_loss = FocalLoss(class_num = 61517)(content_output, content_class)
        return contrastive_loss + (topic_arcface_loss + content_arcface_loss) / 8
    
    def forward(self, topic_input_ids, topic_attention_mask,
                content_input_ids, content_attention_mask,
                topic_class, content_class, label = None):
        topic_embedding = self._feature_generator(topic_input_ids, topic_attention_mask)
        content_embedding = self._feature_generator(content_input_ids, content_attention_mask)
        
        topic_output = self.arcface_topic(topic_embedding, topic_class)
        content_output = self.arcface_content(content_embedding, content_class)
        
        if label is not None:
            loss = self.loss_fn(topic_embedding, content_embedding,
                                topic_output, content_output,
                                topic_class, content_class, label)
        else:
            loss = None
        return loss
        
    def freeze_backbone(self, backbone):
        for param in backbone.parameters():
            param.requires_grad = False

"""# Utils"""

class AWP:
    def __init__(
        self,
        cfg,
        model,
        optimizer,
        adv_param = 'weight',
        adv_lr = 1,
        adv_eps = 0.2,
        start_step = 0,
        adv_step = 1,
        scaler = None
    ):
        self.cfg = cfg
        self.model = model
        self.optimizer = optimizer
        self.adv_param = adv_param
        self.adv_lr = adv_lr
        self.adv_eps = adv_eps
        self.start_step = start_step
        self.adv_step = adv_step
        self.backup = {}
        self.backup_eps = {}
        self.scaler = scaler

    def attack_backward(self, batch, epoch):
        if (self.adv_lr == 0) or (epoch < self.start_step):
            return None

        self._save()
        for i in range(self.adv_step):
            self._attack_step() 
            with autocast(enabled = self.cfg.apex):
                topic_input_ids = batch['topic_input_ids'].to(self.cfg.device)
                topic_attention_mask = batch['topic_attention_mask'].to(self.cfg.device)
                content_input_ids = batch['content_input_ids'].to(self.cfg.device)
                content_attention_mask = batch['content_attention_mask'].to(self.cfg.device)
                topic_class = batch['topic_class'].to(self.cfg.device)
                content_class = batch['content_class'].to(self.cfg.device)
                label = batch['label'].to(self.cfg.device)
                adv_loss = self.model(topic_input_ids, topic_attention_mask, 
                                      content_input_ids, content_attention_mask,
                                      topic_class, content_class, label)
                adv_loss = adv_loss.mean()
            self.optimizer.zero_grad()
            self.scaler.scale(adv_loss).backward()
            
        self._restore()

    def _attack_step(self):
        e = 1e-6
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None and self.adv_param in name:
                norm1 = torch.norm(param.grad)
                norm2 = torch.norm(param.data.detach())
                if norm1 != 0 and not torch.isnan(norm1):
                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)
                    param.data.add_(r_at)
                    param.data = torch.min(
                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]
                    )
                    
    def _save(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None and self.adv_param in name:
                if name not in self.backup:
                    self.backup[name] = param.data.clone()
                    grad_eps = self.adv_eps * param.abs().detach()
                    self.backup_eps[name] = (
                        self.backup[name] - grad_eps,
                        self.backup[name] + grad_eps,
                    )

    def _restore(self,):
        for name, param in self.model.named_parameters():
            if name in self.backup:
                param.data = self.backup[name]
        self.backup = {}
        self.backup_eps = {}

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))

"""# Training functions

* One-epoch training function
"""

def train_fn(cfg, model, train_dataloader, optimizer, epoch, num_train_steps, scheduler, 
             valid_dataloaders, correlations_df, best_score = np.inf):
    # Set up for training
    scaler = GradScaler(enabled = cfg.apex)   # Enable APEX
    loss = 0
    sim_mean = 0
    sim_var = 0
    total_samples = 0
    global_step = 0
    start = end = time.time()

    if cfg.use_awp:
        # Initialize AWP
        awp = AWP(cfg, model, optimizer, adv_lr = cfg.adv_lr, adv_eps = cfg.adv_eps, start_step = cfg.start_awp_epoch, scaler = scaler)

    if cfg.use_tqdm:
        tbar = tqdm(train_dataloader)
    else:
        tbar = train_dataloader
        
    val_schedule = [int(i) for i in list(np.linspace(1, len(tbar), num = int(1 / cfg.val_check_interval) + 1, endpoint = True))[1:]]

    for i, item in enumerate(tbar):
        model.train()
        # Set up inputs
        topic_input_ids = item['topic_input_ids'].to(cfg.device)
        topic_attention_mask = item['topic_attention_mask'].to(cfg.device)
        content_input_ids = item['content_input_ids'].to(cfg.device)
        content_attention_mask = item['content_attention_mask'].to(cfg.device)
        topic_class = item['topic_class'].to(cfg.device)
        content_class = item['content_class'].to(cfg.device)
        label = item['label'].to(cfg.device)
        
        batch_size = content_input_ids.shape[0]

        # Forward
        with autocast(enabled = cfg.apex):
            batch_loss = model(topic_input_ids, topic_attention_mask, 
                               content_input_ids, content_attention_mask, 
                               topic_class, content_class, label)

        if cfg.gradient_accumulation_steps > 1:
            batch_loss = batch_loss / cfg.gradient_accumulation_steps

        # Backward
        scaler.scale(batch_loss).backward()
        
        if cfg.use_awp and epoch >= cfg.start_awp_epoch:
            if epoch == cfg.start_awp_epoch and i == 0:
                print_log(cfg, ' Start AWP '.center(50, '-'))
            if (i + 1) % cfg.gradient_accumulation_steps == 0:
                awp.attack_backward(item, epoch)

        # Update loss
        loss += batch_loss.item() * batch_size
        total_samples += batch_size

        if cfg.use_tqdm:
            tbar.set_description('Batch/Avg Loss: {:.4f}/{:.4f} - '
                                 .format(batch_loss, loss / total_samples))

        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
        if (i + 1) % cfg.gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            global_step += 1
            if cfg.batch_scheduler:
                scheduler.step()

        # Evaluate
        if (i + 1) in val_schedule:
            print_log(cfg, 'Epoch: [{0}][{1}/{2}] - Start evaluating...'.format(epoch + 1, i + 1, len(tbar)))
            oof, val_score, val_recall, val_score_top10, val_recall_top10 = valid_fn(cfg, model, valid_dataloaders, correlations_df)

            end = time.time()
            print_log(cfg, 
                      'Epoch: [{0}][{1}/{2}] - '
                      'Elapsed {remain:s} - '
                      'Train Loss: {train_loss:.4f} - '
                      'Val score/recall: {val_score:.4f}/{val_recall:.4f} - '
                      'Val top10 score/recall: {val_score_top10:.4f}/{val_recall_top10:.4f} - '
                      'LR: {lr:.8f}'
                      .format(epoch + 1, i + 1, len(tbar), 
                              remain = timeSince(start, float(i + 1) / len(tbar)),
                              train_loss = loss / total_samples,
                              val_score = val_score,
                              val_recall = val_recall,
                              val_score_top10 = val_score_top10,
                              val_recall_top10 = val_recall_top10,
                              lr = scheduler.get_lr()[0]))
            if val_recall > best_score:
                best_score = val_recall
                print_log(cfg, f'Epoch [{epoch + 1}][{i + 1}/{len(tbar)}] - The Best Score Updated to: {best_score:.4f} Model')
                ckp = os.path.join(cfg.model_dir, cfg.ver[:-1], cfg.ver[-1])
                model.backbone.save_pretrained(ckp)
            else:
                print_log(cfg, f'Epoch [{epoch + 1}][{i + 1}/{len(tbar)}] - Not The Best Score ({val_recall:.4f}), Current Best Score: {best_score:.4f} Model')
            
    return best_score, oof

"""* One-epoch validation function"""

def infer_embedding_fn(cfg, model, dataloader):
    model.eval()
    
    ids = []
    embeddings = []
    languages = []
    
    if cfg.use_tqdm:
        tbar = tqdm(dataloader)
    else:
        tbar = dataloader

    for i, item in enumerate(tbar):
        batch_ids = item['ids']
        input_ids = item['input_ids'].to(cfg.device)
        attention_mask = item['attention_mask'].to(cfg.device)
        batch_languages = item['language']

        with torch.no_grad():
            with autocast(enabled = cfg.apex):
                batch_embedding = model._feature_generator(input_ids, attention_mask)

        ids.append(batch_ids)
        embeddings.append(batch_embedding.detach().cpu())
        languages.append(batch_languages)

    ids = np.concatenate(ids)
    embeddings = torch.concat(embeddings)
    languages = torch.concat(languages)
    return ids, embeddings, languages

def valid_fn(cfg, model, valid_dataloaders, ground_truth = None, fold = None):
    # Set up for training
    model.eval()

    # Extract the dataloaders
    valid_topic_dataloader, valid_content_dataloader = valid_dataloaders
    
    # Infer the embeddings
    print_log(cfg, 'Extracting topic embeddings...')
    topic_ids, topic_embeddings, topic_languages = infer_embedding_fn(cfg, model, valid_topic_dataloader)
    
    print_log(cfg, 'Extracting content embeddings...')
    content_ids, content_embeddings, content_languages = infer_embedding_fn(cfg, model, valid_content_dataloader)
    
    neighbors_model = NearestNeighbors(n_neighbors = cfg.thres['num_k'], metric = 'cosine')
    neighbors_model.fit(content_embeddings.numpy())
    
    dist, indices = neighbors_model.kneighbors(topic_embeddings.numpy(), return_distance = True)
    
    oof_dict_ids = {}
    oof_dict_ids_top10 = {}
    oof_dict_distance = {}

    for i, (topic_lang, topic_id) in enumerate(zip(topic_languages, topic_ids)):
        chosen_contents = content_ids[indices[i]]
        chosen_contents_top10 = content_ids[indices[i][:10]]

        oof_dict_ids[topic_id] = ' '.join(chosen_contents.tolist())
        oof_dict_ids_top10[topic_id] = ' '.join(chosen_contents_top10.tolist())
        oof_dict_distance[topic_id] = dist[i].tolist()
        
    if ground_truth is not None:
        oof = pd.DataFrame({
            'topic_id': list(oof_dict_ids.keys()),
            'pred_content_ids': list(oof_dict_ids.values()),
            'pred_content_ids_top10': list(oof_dict_ids_top10.values()),
            'pred_distance': list(oof_dict_distance.values()),
        })
        oof = oof.merge(ground_truth[['topic_id', 'content_ids']], on = 'topic_id', how = 'left')
        oof = oof.merge(topics_df[['id', 'fold']], left_on = 'topic_id', right_on = 'id', how = 'left').drop('id', axis = 1)

        if fold is not None:
            score, recall = metric_fn(oof['pred_content_ids'], oof['content_ids'])
            score_top10, recall_top10 = metric_fn(oof['pred_content_ids_top10'], oof['content_ids'])
            print_log(cfg, f'Fold {fold} score/recall/score-top10/recall-top10: {score}/{recall}/{score_top10}/{recall_top10}')
        else:
            for fold in range(cfg.nfolds):
                fold_score, fold_recall = metric_fn(oof.loc[oof.fold == fold, 'pred_content_ids'], 
                                                    oof.loc[oof.fold == fold, 'content_ids'])
                fold_score_top10, fold_recall_top10 = metric_fn(oof.loc[oof.fold == fold, 'pred_content_ids_top10'], 
                                                                oof.loc[oof.fold == fold, 'content_ids'])
                print_log(cfg, f'Fold {fold}: {fold_score}/{fold_recall}/{fold_score_top10}/{fold_recall_top10}')

            score, recall = metric_fn(oof['pred_content_ids'], oof['content_ids'])
            score_top10, recall_top10 = metric_fn(oof['pred_content_ids_top10'], oof['content_ids'])
            print_log(cfg, f'Fold {fold} score/recall/score-top10/recall-top10: {score}/{recall}/{score_top10}/{recall_top10}')

        return oof, score, recall, score_top10, recall_top10
    else:
        return oof_dict_ids, oof_dict_ids_top10, oof_dict_distance

"""* Preparing the optimizer and scheduler"""

def get_optimizer(cfg, model):
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_parameters = [
        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': cfg.encoder_lr, 'weight_decay': cfg.weight_decay},
        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': cfg.encoder_lr, 'weight_decay': 0.0},
        {'params': [p for n, p in model.named_parameters() if 'backbone' not in n],
             'lr': cfg.decoder_lr, 'weight_decay': 0.0}
    ]
    optimizer = AdamW(optimizer_parameters, lr = cfg.lr, eps = cfg.eps, betas = cfg.betas)
    return optimizer

def get_scheduler(cfg, optimizer, num_train_steps):
    if cfg.scheduler_type == 'linear':
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps = cfg.num_warmup_steps, num_training_steps = num_train_steps
        )
    elif cfg.scheduler_type == 'cosine':
        scheduler = get_cosine_schedule_with_warmup(
            optimizer, num_warmup_steps = cfg.num_warmup_steps, num_training_steps = num_train_steps, num_cycles = cfg.num_cycles
        )
    return scheduler

"""* Training-loop function"""

def training_loop(cfg):
    print_log(cfg, 'Preparing the training and validation dataloaders...')
    dataset = LECRDataset(cfg, data, topic_data, content_data)
    dataloader = DataLoader(dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = True)
    
    valid_correlations_df = pd.read_csv(os.path.join(cfg.comp_data_dir, 'correlations.csv'))
    
    valid_topic_dataset = LECR_ComponentDataset(cfg, topics_df.loc[(topics_df['category'] != 'source') & 
                                                                    topics_df.has_content])
    valid_content_dataset = LECR_ComponentDataset(cfg, content_df)
    
    valid_topics_dataloader = DataLoader(valid_topic_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)
    valid_content_dataloader = DataLoader(valid_content_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)
    valid_dataloaders = (valid_topics_dataloader, valid_content_dataloader)

    print_log(cfg, 'Preparing the model, optimizer, and scheduler...')
    model = LECRModel(cfg).to(cfg.device)
    optimizer = get_optimizer(cfg, model)
    num_training_steps = len(dataloader) * cfg.nepochs
    scheduler = get_scheduler(cfg, optimizer, num_training_steps)

    best_score = -np.inf
    for epoch in range(cfg.nepochs):
        start_time = time.time()
        # Train
        best_score, oof = train_fn(cfg, model, dataloader, optimizer, epoch, num_training_steps, scheduler, 
                                   valid_dataloaders, valid_correlations_df, best_score = best_score)
    return oof

"""# Main"""

def main():
    oofs = training_loop(cfg)
    score = metric_fn(oofs['pred_content_ids'], oofs['content_ids'])
    print_log(cfg, f'Overall score: {score}')
    # Storing OOF file
    oofs.to_pickle(os.path.join(cfg.model_dir, cfg.ver[:-1], cfg.ver[-1], f"oof_{''.join([str(i) for i in cfg.training_folds])}.pkl"))

if __name__ == '__main__':
    main()